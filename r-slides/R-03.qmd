---
title: "Computational Text Analysis"
subtitle: "**Module 03** | GESIS Fall Seminar \"Introduction to Computational Social Science\""
author:
  - name: Johannes B. Gruber
    affiliations:
      - name: VU Amsterdam
  - name: John McLevey
    affiliations:
      - name: University of Waterloo
format:
  revealjs: 
    # the theme increase the file size 3-5x and makes rendering much slower, I replaced it with a similar one while working
    theme: [default, custom.scss]
    # theme: moon
    width: 1600
    height: 900
    embed-resources: true
    execute:
      echo: true
      cache: true
    slide-number: false
    chalkboard: false
    preview-links: auto
    smaller: false
    fig-align: left
    fig-format: svg
    lightbox: true
    scrollable: true
    code-overflow: scroll
    code-fold: false
    code-line-numbers: true
    code-copy: hover
    code-block-border-left: gray 
    code-block-bg: slate
    reference-location: document
    logo: media/logo_gesis.png
    # footer: "*Link to Live Course Website Here*"
    email-obfuscation: javascript
highlight-style: "nord"
bibliography: references.bib
---

## Schedule: GESIS Fall Seminar in Computational Social Science

| time   | Session                                      |
|--------|----------------------------------------------|
| Day 1  | Introduction to Computational Social Science |
| Day 2  | Obtaining Data                               |
| **Day 3**  | **Computational Text Analysis**                  |
| Day 4  | Computational Network Analysis               |
| Day 5  | Social Simulation & Agent-based Models       |
| Day 6  | Project Work Day and Outlook                 |

: Course Schedule {.striped .hover}

<!-- This is a comment. You can leave them here to take notes linked to the slides. -->

## Plan for today

- What is Computational Text Analysis?
- Dictionary-based Approaches
- Supervised Machine Learning
- Unsupervised Machine Learning
- Word Embeddings
- generative Large Language Models

# What is Computational Text Analysis?
## Basics

- Definition: Computational Text Analysis involves using computer algorithms and techniques to process, analyse, and derive insights from textual data
- Data Sources: Textual data can come from various sources such as social media posts, news articles, academic papers, emails, forums, and other digital communications

## Text/Content Analysis in Social Science

- Researchers have long recognised that much of what is social interaction is expressed through words
- Traditionally these texts are analysed using content analysis in one of several forms (e.g., Quantitative Content Analysis, Discourse Analysis or Hermeneutic Content Analysis)
- The goal was to understand what actors are saying and writing and sometimes why they might say or write something
- The steps could involve theory/category building, classification of text and finding relations between texts/actors
- But scholars struggle with volume: ***there are often simply too many relevant texts***
- At least linear increase of costs with larger text corpora (structured collections of text)

## Promises of Automatic Content Analysis

- *Systematic analysis* of large-scale text collections without massive funding support
- Depending on the method, results are available almost immediately
- Perfect reliability in the sense that presented the same data and method, the computer will generate same results

## Pitfalls of Automatic Content Analysis: Four Principles [@grimmer_text_2013]

1. All quantitative models of language are wrong—but some are useful 
2. Quantitative methods augment humans, not replace them
3. There is no globally best method for automated text analysis
4. Validate, Validate, Validate!
- Problem with 4: The discipline is still young and validation methods are often not canonical or do not exist yet.

## text-as-data methods

![](media/R-03/tada.png)

From @grimmer_text_2013

## text-as-data methods

![](media/R-03/stock.png)

From @boumans_taking_2015

## text-as-data methods

![](media/R-03/optimal_tada_method.png){width=600}

From myself.

# Dictionary-based Approaches
## What are Dictionary-based Approaches?

:::: {.columns}

::: {.column width="70%"}
- Lexicon/Dictionary: the words in a language and their meaning
- Lexicon/Dictionary-based approaches: simply count how often pre-defined words appear to infer meaning of text
- Wordcounts are usualy used to categorise text (e.g., non-/relevant, positive/negative, a-/political)
- To infer category from count, researchers define mapping function (e.g., N positive terms > N negative terms = positive text) 
- Like 'normal' dictionaries: several forms of the word carry same meaning, expressed through wildcards (e.g., econom*) or regular expressions (e.g., econom.+) (matches economists, economic, and so on)
:::

::: {.column width="30%"}
![](media/R-03/lexicon.jpeg)
:::

::::

## Why choose Lexicon/Dictionary-based Approaches?

- Fully transparent even without technical knowledge
- Lightweight to run, even on enormous data sets
- Easy to implement it for nonconsumptive research (e.g., Google Books let's you search, but not read/consume books)
- Valid choice under 3 conditions (@cssbook):
  1. Variable we want to code is <u>manifest</u> and <u>concrete</u> rather than latent and abstract: names of actors, specific physical objects, specific phrases, etc., rather than feelings, frames, or topics.
  2. All synonyms to be included must be known beforehand.
  3. And third, the dictionary entries must not have multiple meanings.

## Setup Environment

- Switch to Quarto View: <https://github.com/mclevey/computational-social-science-python-r/blob/main/r-slides/R-03.qmd>

```{r setup}
library(ngramr)
library(tidyverse)
library(tidytext)
library(yardstick)
```

## Example: Non-Consupmtive Research with Google Books

Taken from @duneier_ghetto_2017: Ghetto: The Invention of a Place, the History of an Idea 

**RQ**: How did the meaning of ghetto change over time?
**Method**: Non-Consumptive Research with the Google Books Ngram Viewer

```{r}
#| output-location: column
#| echo: true
#| fig-asp: 1
ng  <- ngram(
  phrases = c("ghetto", 
              "(Warsaw ghetto) + (Jewish ghetto)", 
              "(black ghetto) + (negro ghetto)"), 
  year_start = 1920,
  year_end = 1975,
  smoothing = 0,
  count = TRUE
) |> 
  group_by(Year) |> 
  mutate(pct = Count / Count[Phrase == "ghetto"]) |> 
  filter(Phrase != "ghetto")

ggplot(ng, aes(x = Year, y = pct, colour = Phrase)) +
  geom_line() +
  theme(legend.position = "bottom")
```

## Example 2: Sentiment Analysis

This part is taken from [@cssbook Chapter 11.2](https://cssbook.net/content/chapter11.html#sec-reviewdataset).
We first get the data for this, which consists of movie reviews from the IMDB database [@aclimdb].

```{r}
# You can ignore this part where I download and process the data. The code is
# not really applicable for other dataset. But I left it in here in case you
# find it interesting.
data_file <- "data/R-03/imdb.rds"
if (!file.exists(data_file)) {
  message("Downloading data")
  temp <- file.path(tempdir(), "imdb") 
  dir.create(temp, recursive = TRUE)
  curl::curl_download("https://cssbook.net/d/aclImdb_v1.tar.gz",
                      file.path(temp, "imdb.tar.gz"), quiet = FALSE)
  untar(file.path(temp, "imdb.tar.gz"), exdir = temp)
  files <- list.files(temp, 
                      pattern = ".txt", 
                      recursive = TRUE,
                      full.names = TRUE)
  imdb <- map(files, function(f) {
    tibble(
      file = f,
      text = readLines(f, warn = FALSE)
    )
  }) |> 
    bind_rows() |> 
    mutate(label = str_extract(file, "/pos/|/neg/"),
           label = str_remove_all(label, "/"),
           label = factor(label)) |>
    filter(!is.na(label)) |>
    select(-file) |> 
    # adding unique IDs for later
    mutate(id = row_number())
  saveRDS(imdb, data_file)
} else {
  message("Using cached data")
  imdb <- readRDS(data_file)
}
```

## Getting a dictionary

We download a dictionary from the *Computational Analysis of Communication* website [@cssbook], which consists of a list of positive, and one list of negative words.

```{r dict-r}
poswords <- "https://cssbook.net/d/positive.txt"
negwords <- "https://cssbook.net/d/negative.txt"
sentiment_dict <- bind_rows(
  tibble(word = scan(poswords, what = "character"), value = 1),
  tibble(word = scan(negwords, what = "character"), value = -1)
)
sentiment_dict[c(1:5, 5660:5664), ]
```

## Apllying dictionary

We then go through all reviews and construct a sentiment score by looking up each word and adding up its score:

```{r sentsimple-r}
scores_df <- imdb |> 
  # For speed, we only take the first 100 reviews
  head(100) |> 
  # This splits up the texts into its individual words
  unnest_tokens(output = "word", input = "text", drop = FALSE) |> 
  # We attach the sentiment_dict to the text data.frame. inner_join drops
  # rows where the word is not in both data.frames
  inner_join(sentiment_dict, by = "word") |>
  # For each text, we calcuate the sum of values
  group_by(id) |> 
  summarise(senti_score = sum(value),
            text = head(text, 1))

head(scores_df)
```

## Applying dictionary

More commonly, people normalize the absolute count of positive and negative words to construct a score:

- -1 (most negative sentiment) to +1 (most positive sentiment).
- mapping function
  - N positive terms >= N negative terms = positive text
  - N positive terms < N negative terms = negative text

```{r sentnorm-r}
scores_df <- imdb |> 
  # For speed, we only take the first 100 reviews
  head(100) |>  
  unnest_tokens(output = "token", input = "text", drop = FALSE) |> 
  inner_join(sentiment_dict, by = c("token" = "word")) |> 
  group_by(id) |>
  # here, we normalise the outcome and assign a sentiment category
  summarise(senti_score = sum(value) / n(),
            sentiment = ifelse(senti_score >= 0, "pos", "neg"),
            text = head(text, 1))

head(scores_df)
```

## Plotting the scores

We can plot these results to get an impression how often each category was predicted and how strong the senti_score was in these cases.

```{r sent-plot-r}
scores_df |> 
  mutate(id = fct_reorder(as.character(id), senti_score)) |> 
  ggplot(aes(x = senti_score, y = id, fill = sentiment)) +
  geom_col() +
  labs(y = NULL, fill = NULL)
```

## Validation

We can validate this approach by comparing the measured sentiment to the real sentiment, as given in the dataset:

```{r}
validation_df <- scores_df |> 
  select(-text) |> 
  left_join(imdb, by = "id")

# have a look at the new data.frame
validation_df |> 
  select(id, label, sentiment, senti_score)
```

An easy way to validate performance is to calculate how often the prediction and the real sentiment match:

```{r}
validation_df |> 
  count(match = label == sentiment)
```

However, the absolute count of matches, or accuracy, is prone to errors, since it does not take into account chance.
For example, by taking only the first 100 reviews, we happen to have gather data that has just negative cases:

```{r}
validation_df |> 
  count(label)
```

So while optimising our mapping function we could accidentally make a wrong adjustment:

```{r}
scores_df_error <- imdb |> 
  head(100) |>  
  unnest_tokens(output = "token", input = "text", drop = FALSE) |> 
  inner_join(sentiment_dict, by = c("token" = "word")) |> 
  group_by(id) |>
  summarise(senti_score = sum(value) / n(),
            sentiment = ifelse(senti_score >= 0, "neg", "neg"),
            #                 see the error here --^
            text = head(text, 1))

scores_df_error |> 
  select(-text) |> 
  left_join(imdb, by = "id") |> 
  count(match = label == sentiment)
```

Now suddenly our accuracy is perfect!
This is why we use a couple of metrics that control for chance:

```{r}
conf_matrix <- table(validation_df$label, validation_df$sentiment)
ml_metrics <- metric_set(accuracy, precision, recall, f_meas, kap)
conf_matrix
ml_metrics(conf_matrix)
```

## Issues

![](media/R-03/136.png)

- The more terms we add to our dictionary, the more false positives we will get
- Building a good dictionary is a lot of work (complexity-resource plot):
  - Negation and bag-of-word issues ("not good" will be counted as positive + modifiers such as "very good")
  - "great" should be more positive than "good"
- Negative image of dictionaries in academia
  - Many negative examples where dictionaries were applied often outside of the domain they had been developed
  - Wrong believe that popular off-the-shelf dictionaries do not need validation
  - Many papers that show that dictionaries do not perform as well as machine learning: e.g. @van_atteveldt_validity_2021; @bailon_signals_2015; @boukes_whats_2020

Now that you know about dictionaries, remember to apply them only under some circumstances:

  0. When no other method is available, e.g., in data retrieval or nonconsumptive research
  1. Variable we want to code is manifest and concrete rather than latent and abstract: names of actors, specific physical objects, specific phrases, etc., rather than feelings, frames, or topics.
  2. All synonyms to be included must be known beforehand.
  3. And third, the dictionary entries must not have multiple meanings.


# Supervised Machine Learning
## What are Supervised Machine Learning Approaches?

![](media/R-03/machine-learning-0.png)

## What are Supervised Machine Learning Approaches? {visibility="uncounted"}

![](media/R-03/machine-learning-1.png)

## Steps in SML

We proceed in 4 (or 5) steps:

1. preprocessing the incoming text
2. splitting the dataset into training and a test set (which is not included in the model and just used for validation)
3. fitting (or training) the model 
4. using the test set to compare predictions against the real values for validation
5. (using the model on new documents)

## Why choose Supervised Machine Learning Approaches?

- Usually more accurate than dictionaries
- Straightforward way to construct and validate models
- Even when models are not valid (enough) you still have hand coded data that you can fall back on

## What should you use?

:::: {.columns}

::: {.column width="50%"}
Several frameworks in R:

- `RTextTools`: includes the most important algorithms, but outdated
- `caret`: includes a huge variety of models and comes with robust helper functions, but superseded
- `quanteda`: nice suite of text analysis functions, including `quanteda.textmodels`
- `tidymodels`: successor of `caret` with an extensive collection of algorithms, preprocessing, validation and comparison functions
:::

::: {.column .fragment width="50%"}
![](https://jhudatascience.org/tidyversecourse/images/book_figures/simpletidymodels.png)
:::

::::

## 1. Using `textrecipes` to turn text into features

- `textrecipes`  (an extension of the `recipes` package): handles all preprocessing in a unified syntax
- syntax is similar for a large variety of machine learning (including, e.g., OLS or logisitc regression)

Define recipe:

```{r}
library(tidymodels)
library(textrecipes)
imdb_rec <- recipe(label ~ text, data = imdb) |>
  # turn text into features/tokens/words
  step_tokenize(all_predictors()) |>
  # remove stopwords
  step_stopwords(language = "en") |> 
  step_tokenfilter(all_predictors(), min_times = 3) |>
  step_tf(all_predictors())
```

Bake recipe with ingredients (data):

```{r}
imdb_rec |> 
  prep(head(imdb, 10)) |>
  bake(new_data = NULL)
```

::: {.callout-note}
Stopwords are words that are thought to add no meaning to a text and can be safely removed. 
Many text analysis packages come with their own selection, but whether it's useful depends on context!
You can look at `stopwords::stopwords()` to see one selection.
:::

## 2. Splitting the dataset {#splitting-the-dataset}

- important to leave out a portion of the data for validation:

```{r}
set.seed(1)
split <- initial_split(
  data = imdb, 
  prop = 3 / 4,   # the prop is the default, I just wanted to make that visible
  strata = label  # this makes sure the prevalence of labels is still the same afterwards
) 
imdb_train <- training(split)
imdb_test <- testing(split)
```

::: {.callout-important}
Making the split only once is not good practice, as which texts end up in the training and test portion of the data can influence the performance metrics of the model.
Some researchers have even been caught trying many different seeds to find one that delivers better looking results.
What you should do instead is to use the `bootstraps()` to fit multiple different combinations.
Check out one of [Julia Silge's videos](https://youtu.be/z57i2GVcdww) if you want to learn more.
:::


## 3. Fitting a model {#fitting-a-model}

Let's start with a naïve bayes model, for which we need another package which contains this "engine" (you don't need to know this, the next step would tell you to load the package):

```{r}
library(discrim)
nb_spec <- naive_Bayes() |>
  set_mode("classification") |>
  set_engine("naivebayes")
```

Now we bring the recipe and model together in a new workflow:

```{r}
imdb_wf_nb <- workflow() |> 
  add_recipe(imdb_rec) |> 
  add_model(nb_spec)
```

Now, we fit the model:

```{r}
model_nb <- fit(imdb_wf_nb, data = imdb_train)
```

## 4. Evaluating the model {#evaluating-the-model}

```{r}
imdb_prediction <- imdb_test |> 
  bind_cols(predict(model_nb, new_data = imdb_test)) |>
  rename(truth = label, estimate = .pred_class)

conf_mat(imdb_prediction, truth, estimate)
```

```{r}
library(gt)
my_metrics <- metric_set(accuracy, kap, precision, recall, f_meas)

my_metrics(imdb_prediction, truth = truth, estimate = estimate) |> 
  # I use gt to make the table look a bit nicer, but it's optional
  gt() |> 
  data_color(
    columns = .estimate,
    fn = scales::col_numeric(
      palette = c("red", "orange", "green"),
      domain = c(0, 1)
    )
  )
```

## quick explanation of the common metrics

1. **Precision**: This is the ratio of correctly predicted positive observations to the total predicted positives. High precision means that an algorithm returned more relevant results than irrelevant ones.

2. **Recall** (Sensitivity): This is the ratio of correctly predicted positive observations to the all observations in actual class. High recall means that an algorithm returned most of the relevant results.

3. **F1 Score**: This is the weighted average of Precision and Recall. It tries to find the balance between precision and recall. High F1 score means that both recall and precision are high.

We often use a confusion matrix to calculate these metrics. A confusion matrix is a 2x2 table that contains 4 outputs provided by the binary classifier. The terminology can vary, but it's often formatted as follows:

|                    | Actual Positive     | Actual Negative     |
|--------------------|---------------------|---------------------|
| Predicted Positive | True Positive (TP)  | False Positive (FP) |
| Predicted Negative | False Negative (FN) | True Negative (TN)  |

Based on this matrix:

- Precision = TP / (TP + FP)
- Recall = TP / (TP + FN)
- F1 Score = 2*(Recall * Precision) / (Recall + Precision)

## A quick example

Let's say we have a binary classifier that's being used to predict whether a given email is "spam" (positive class) or "not spam" (negative class).
Suppose our classifier is tested on a dataset of 100 emails, which include 20 actual spam messages and 80 actual non-spam messages. The classifier outputs the following results:

|                    | Actual Spam     | Actual Not Spam |
|--------------------|-----------------|-----------------|
| Predicted Spam     | 15 (TP)         | 5 (FP)          |
| Predicted Not Spam | 5 (FN)          | 75 (TN)         |

Here, the precision, recall, and F1 score would be calculated as:

- Precision = TP / (TP + FP) = 15 / (15 + 5) = 0.75
- Recall = TP / (TP + FN) = 15 / (15 + 5) = 0.75
- F1 Score = 2*(Recall * Precision) / (Recall + Precision) = 2*(0.75 * 0.75) / (0.75 + 0.75) = 0.75

So in this case, the classifier has a precision of 0.75 (meaning that 75% of the emails it labeled as "spam" were actually spam), a recall of 0.75 (meaning that it correctly identified 75% of the total spam emails), and an F1 score of 0.75 (giving a balance between precision and recall).
Which of these metrics is most useful depends on the task.
If you detect cancer in patients, false positives are not great, but false negatives might be deadly!
In this case you should optimise the **recall** and only look at the other metrics as an afterthought.
However, most of the time, we want to get a good F1 value (rule of thumb is above 0.7 or better 0.8).
Note that in most cases, we do not really care about a "positive" class as both classes are equally important.
And often, we have more than 2 classes.
A good strategy here is to calculate F1 for each class and reporting all values plus the average.


## Alternative models

Let's walk through the steps again to train a logistic regression model:

```{r}
# choosing the model
lasso_spec <- logistic_reg(penalty = 0.01, mixture = 1) |>
  set_mode("classification") |>
  set_engine("glmnet")

# bring preprocessing and model together in a workflow
imdb_wf_lasso <- workflow() |> 
  add_recipe(imdb_rec) |> 
  add_model(lasso_spec)

# train the model
model_lasso <- fit(imdb_wf_lasso, data = imdb_train)

# evaluate
imdb_prediction_lasso <- imdb_test |> 
  bind_cols(predict(model_lasso, new_data = imdb_test)) |> 
  rename(truth = label, estimate = .pred_class)

my_metrics <- metric_set(accuracy, kap, precision, recall, f_meas)

my_metrics(imdb_prediction_lasso, truth = truth, estimate = estimate) |> 
  gt() |> 
  data_color(
    columns = .estimate,
    colors = scales::col_numeric(
      palette = c("red", "orange", "green"),
      domain = c(0, 1)
    )
  )
```

## Additional step: looking inside the model

A way to make sense of a model is to look at coefficients.
This tells us essentially, what the model thinks are important terms to say a document is positive or negative.

```{r}
model_lasso |> 
  extract_fit_engine() |> 
  vip::vi() |>
  group_by(Sign) |>
  slice_max(Importance, n = 20) |> 
  ungroup() |>
  mutate(
    Variable = str_remove(Variable, "tf_text_"),
    Variable = fct_reorder(Variable, Importance)
  ) |>
  ggplot(aes(x = Importance, y = Variable, fill = Sign)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~Sign, scales = "free_y") +
  labs(y = NULL)
```


## Issues

- Many training documents needed to build a valid model
- Even then, there is no guarantee you get good evaluation results
- Many choices in terms of models and preprocessing, but no one-model-fits all
- Models are specific to the training data and often will not work in other domains

## Learn more

:::: {.columns}

::: {.column width="70%"}
- Emil Hvitfeldt, Julia Silge: *Supervised Machine Learning for Text Analysis in R*, https://smltar.com/
:::

::: {.column width="30%"}
![](https://www.tidymodels.org/books/smltar/cover.png)
:::

::::

# Unsupervised Machine Learning
## What are Unsupervised Machine Learning Approaches?

- The goal of topic modelling is to automatically assign topics to documents without requiring human supervision
- Explore text collection by letting the computer figure out which words and which texts belong together
- Commonly Latent Dirichlet Allocation is used for probabilistic topic modelling (although many consider BERTopic to be better now)
- Newer alternatives like BERTopic improve 
- Although the idea of an algorithm figuring out topics might sound close to magical (mostly because people have too high expectations of what these 'topics' are), and the mathematics might be a bit challenging, it is actually really simple fit an LDA topic model in R

## Playful exploration^[heavily influenced by this piece: https://medium.com/\@lettier/how-does-lda-work-ill-explain-using-emoji-108abf40fa7d]

A good first step towards understanding what topic models are and how they can be useful, is to simply play around with them, so that's what we'll do first.
Open the page <https://lettier.com/projects/lda-topic-modeling/> (if possible, in Firefox)

Example Docs:

1.  🐭 🐭 🐭 🐭 🐭 🐭 🐭 🐭 🐭 🐭
2.  🐱 🐱 🐱 🐱 🐱 🐱 🐱 🐱 🐱 🐱
3.  🐶 🐶 🐶 🐶 🐶 🐶 🐶 🐶 🐶 🐶
4.  🐭 🐭 🐭 🐭 🐭 🐭 🐭 🐭 🐭 🐭 🐱 🐱 🐱 🐱 🐱 🐱 🐱 🐱 🐱 🐱 🐶 🐶 🐶
    🐶 🐶 🐶 🐶 🐶 🐶 🐶

On top, the texts are turned into a document feature matrix.

![](media/R-03/lda-dfm.png)

What LDA does is essentially to reduce the dimensions of a text into a set of 'topics' that are easier to interpret.
It then expresses the relation between cases and topics, as well as variables and topics in two matrices.
If you have used PCA, MDS or factor analysis before, this is essentially the same process.

The first matrix describes the probability a feature belongs to a topic.
We call this the feature-topic-matrix.

![](media/R-03/lda-ftm.png)

The second table describes does the same, but with documents.
We call this the document-term-matrix.

![](media/R-03/lda-dtm.png)

This case makes it pretty clear: mice belong to the first topic, cats belong to the second, dogs belong to the third topic.
The first document has a high probability to belong to the first topic, because it is full of mice.
The second document has a high probability to belong to the second topic, because it is full of cats.
The third document has a high probability to belong to the third topic, because it is full of dogs.

What makes LDA often seem magical, is how well it works for text.
This is because the underlying assumptions fit the statistical features of document collections quite well, leading to meaningful and interpretable categories that make it easy to explore and summarise what is happening in text.

The short example above shows the three broad steps of topic modelling:

1.  Create a document-feature-matrix from your documents (and preprocess it)
2.  Fit the topic model
3.  Analyze (and validate) the results

Before we go on, here are a few things I want you to try:

1. Change the Alpha and Beta values
2. Add and replace your own text
3. Change the number of topics

## (0) Obtaining the data

We use a subset of the Parlspeech corpus [@parlspeech2020], spanning the 18th legislative period of the Bundestag.

```{r}
# Again, you can ignore this part where I download and process the data. The code is
# not really applicable for other dataset. But I left it in here in case you
# find it interesting..
data_file <- "data/R-03/bundestag18_speeches.rds"
if (!file.exists(data_file)) {
  library(dataverse)
  Sys.setenv("DATAVERSE_SERVER" = "dataverse.harvard.edu")
  ds <- get_dataset("doi:10.7910/DVN/L4OAKN")
  as_tibble(ds[["files"]][, c("label", "id")])
  
  url <- get_file(3758785, 
                  dataset = "doi:10.7910/DVN/ZY3RV7",
                  return_url = TRUE)
  
  curl::curl_download(url, destfile = "data/R-03/Corp_Bundestag_V2.rds", quiet = FALSE)
  
  bundestag18 <- readRDS("data/R-03/Corp_Bundestag_V2.rds") |>
    mutate(date = ymd(date),
           speechnumber = as.integer(speechnumber)) |>
    filter(date >= "2013-10-22",
           date <= "2017-10-24") |>
  mutate(doc_id = paste0(date, "-", speechnumber)) |>
  as_tibble()
  saveRDS(bundestag18, "data/R-03/bundestag18_speeches.rds")
} else {
  bundestag18 <- readRDS("data/R-03/bundestag18_speeches.rds")
}
```

## (1) Creating a DFM

We first tidy the documents:

```{r unnest_bundestag18, cache.lazy=FALSE}
bundestag18_tidy <- bundestag18 |>  
  unnest_tokens(output = word, input = "text")
```

Secondly, we do some light cleaning:

- removing stopwords
- removing rare terms
- removing features that aren't words

```{r, cache.lazy=FALSE}
bundestag18_tidy_clean <- bundestag18_tidy |>
  filter(!word %in% c(stopwords::stopwords(language = "de"), 
                      "dass", "kollege", "kollegin", "herr", "frau", "dr")) |>
  group_by(word) |>
  filter(
    n() > 10,                    # keep features that appear more than 10 times
    !str_detect(word, "[^a-z]")  # keep features that consist only of characters
  ) |>
  ungroup()

print(glue::glue(
  "Cleaning removed {length(unique(bundestag18_tidy$word)) - length(unique(bundestag18_tidy_clean$word))} ",
  "unique words and ",
  "{length(unique(bundestag18_tidy$doc_id)) - length(unique(bundestag18_tidy_clean$doc_id))} documents. ",
  "{length(unique(bundestag18_tidy$word))} unique words remain in ",
  "{length(unique(bundestag18_tidy$doc_id))} documents"
))
```

Now we can create a document-feature-matrix.
This is a (sparse) matrix showing how often each term (column) occurs in each document (row):

```{r, cache.lazy=FALSE}
bundestag18_dfm <- bundestag18_tidy_clean %>%
  count(doc_id, word) |>
  cast_dfm(doc_id, word, n)
```

We can inspect a corner of the dfm by casting it to a regular (dense) matrix:

```{r}
as.matrix(bundestag18_dfm[1:5, 1:5])
```

## (2) Running the topic model {#running-the-topic-model}

- many different ways to fit a topic model (`topicmodels` or `lda` package, stay away from `mallet`, try BERTopic if you know a little python)
- I use `textmodel_lda()` function from the `seededlda` package due to the clean syntax

```{r seededlda}
library(seededlda)
k <- 10
set.seed(1) # Note, that we use `set.seed` to create reproducible results
lda_model <- textmodel_lda(
  bundestag18_dfm,
  k = k,              # the number of topics is chosen at random for demonstration purposes
  max_iter = 200L,    # I would not usually recommend that few iterations, it's just so it runs quicker here
  alpha = 50L / k,    # these are the default values in the package
  beta = 0.1,
  verbose = TRUE
)
```

## (3) Inspecting and analysing the results {#inspecting-and-analysing-the-results}
### Word-topic probabilities {#word-topic-probabilities}

```{r, fig.width=8, fig.height=16}
bundestag18_ftm <- lda_model$phi |>
  as.data.frame() |># converting the matrix into a data.frame makes sure it plays nicely with the tidyverse
  rownames_to_column("topic") |># the topic names/numbers are stored in the row.names, I move them to a column
  mutate(topic = fct_inorder(topic)) |># turn to factor to maintain the correct order
  pivot_longer(-topic, names_to = "word", values_to = "phi")

topic_topwords_plot <- bundestag18_ftm |># turn to long for plotting
  group_by(topic) |># using group_by and slice_max, we keep only the top 10 values from each topic
  slice_max(order_by = phi, n = 15) |>
  # using reorder_within does some magic for a nicer plot
  mutate(word = tidytext::reorder_within(word, by = phi, within = topic)) |>
  # from here on, we just make a bar plot with facets
  ggplot(aes(x = phi, y = word, fill = topic)) +                               
  geom_col() +
  tidytext::scale_y_reordered() +
  facet_wrap(~topic, ncol = 2, scales = "free_y")
topic_topwords_plot
```


::: {.fragment}
Going forward, I would now name these topics.
I found this particular format in a Excel sheet helpful.

```{r}
lda_model$phi |>
  as.data.frame() |>
  rowid_to_column("topic") |>
  pivot_longer(-topic, names_to = "word", values_to = "phi") |>
  group_by(topic) |>
  slice_max(order_by = phi, n = 20) |>
  mutate(top = row_number()) |>
  pivot_wider(id_cols = top, names_from = topic, values_from = word) |>
  # Add an extra row where you can write in topic names
  add_row(top = NA, .before = 1) %>%
  rio::export("7._topicsmodel_topwords.xlsx")
```
:::

## Topics per document {#topics-per-document}

Similarly to above, we can also extract to topics per document:

```{r}
bundestag18_dtm <- lda_model$theta |>
  as.data.frame() |>
  rownames_to_column("doc_id") |>
  as_tibble()
bundestag18_dtm
```

We can tidy this and join the results back with the original metadata using the `doc_id`:

```{r}
bundestag18_dtm_tidy <- bundestag18_dtm |>
  pivot_longer(-doc_id, names_to = "topic", values_to = "theta") |>
  # again this is to keep track of the order as it is otherwise order by alphabet
  mutate(topic = fct_inorder(topic)) |>
  left_join(bundestag18 |>select(-text), by = "doc_id")
bundestag18_dtm_tidy
```

Now, we can e.g. compare topic usage per party:

```{r}
bundestag18_dtm_tidy |> 
  filter(!is.na(party),
         party != "independent") |>
  group_by(party, topic) |> 
  summarize(theta = mean(theta)) %>%
  ggplot(aes(x = theta, y = topic, fill = party)) + 
  geom_col(position = "dodge")  +
  scale_fill_manual(values = c(
    "PDS/LINKE" = "#BD3075",
    "SPD" = "#D71F1D",
    "GRUENE" = "#78BC1B",
    "CDU/CSU" = "#121212",
    "FDP" = "#FFCC00",
    "AfD" = "#4176C2"
  ))
```

Or over time:

```{r}
bundestag18_dtm_tidy |>
  group_by(date = floor_date(date, "months"), topic) |> 
  summarize(theta = mean(theta)) |>
  ggplot(aes(x = date, y = theta, colour = topic)) +
  geom_line()
```

```{r}
bundestag18_dtm_tidy |>
  group_by(date = floor_date(date, "months"), topic) |> 
  summarize(theta = mean(theta)) |>
  ggplot(aes(x = date, y = theta, colour = topic)) +
  geom_line() +
  facet_wrap(vars(topic))
```


## Some alternative ways to explore the model

### LDAvis

A popular way of exploring topics and how they overlap is the `LDAvis` package.

```{r eval=FALSE}
#| eval: false
library(LDAvis)
json <- createJSON(phi = lda_model$phi,
                   theta = lda_model$theta, 
                   doc.length = quanteda::ntoken(lda_model$data),
                   vocab = quanteda::featnames(lda_model$data), 
                   term.frequency = quanteda::featfreq(lda_model$data))
serVis(json)
```

LDAvis helps users understand the relationships between topics and the key terms that define them.


### `tokenbrowser`

We can check how the probabilities for documents are calculated by looking at the words and topic probabilities in their original context using the using the [tokenbrowser](https://github.com/kasperwelbers/tokenbrowser) package developed by Kasper Welbers.

We first select the 2000 tokens with the highest phi value, i.e., the ones which most clearly belong to one topic.

```{r}
categories <- bundestag18_ftm %>%
  group_by(word) |>
  mutate(phi_rel = phi / sum(phi)) |>
  slice_max(order_by = phi, n = 1) |>
  ungroup() |>
  filter(phi_rel >= 0.5)
```

Then we attach the categories to the original tidy representation of the texts.

```{r}
assignments <- bundestag18_tidy |>
  filter(doc_id %in% unique(bundestag18_tidy$doc_id)[1:5]) |>
  left_join(categories, by = "word")
```

Now we can look at the words that clearly belong to a topic in context of the full speeches.

```{r}
#| eval: false
library(tokenbrowser)
categorical_browser(
  assignments,
  category = as.factor(assignments$topic), 
  token_col = "word"
) |>
  browseURL()
```

# Finding an optimal number of topics

- The best way to find the optimal $k$ number of topics is to interpret different models and look for the ones that seems to divide your corpus into the most meaningful topics
- That is very cumbersome though and there are some statistical methods to make the process easier
- The idea behind all of them is to compare the metrics of different models to narrow your search down

```{r seededlda_batch}
library(furrr)
lda_models <- "data/R-03/lda_models.rds"
# since this takes a long time, I save the results in case I want to run it again
if (!file.exists(lda_models)) {
  plan(multisession)  ## for parallel processing
  lda_fun <- function(k, max_iter = 200) {
    textmodel_lda(
      bundestag18_dfm,
      k = k,
      max_iter = max_iter,
      alpha = 50 / k,
      beta = 0.1,
      verbose = TRUE
    )
  }
  
  models_df <- tibble(
    k = c(10:20),
    model = future_map(k, lda_fun, .options = furrr_options(seed = 1))
  )
  
  saveRDS(models_df, lda_models)
} else {
  models_df <- readRDS(lda_models)
}
```

There is no official function in `seededlda` to evaluate different models.
The `stm` package is much better here [as demonstrated by Julia Silge](https://juliasilge.com/blog/evaluating-stm/).
But since I did not want to introduce another package, I copied the functions that are currently discussed from [this issue on GitHub](https://github.com/koheiw/seededlda/issues/26).

```{r}
semantic_coherence <- function(model, top_n = 10) {
    h <- apply(terms(model, top_n), 2, function(y) {
        d <- model$data[,y]
        e <- Matrix::Matrix(docfreq(d), nrow = nfeat(d), ncol = nfeat(d))
        f <- fcm(d, count = "boolean") + 1
        g <- Matrix::band(log(f / e), 1, ncol(f))
        sum(g)
    })
    sum(h)
}

divergence <- function(model) {
    div <- proxyC::dist(model$phi, method = "kullback")
    diag(div) <- NA
    mean(as.matrix(div), na.rm = TRUE)
}

# this one is taken from stm https://github.com/bstewart/stm/blob/master/R/exclusivity.R
exclusivity <- function(model, top_n = 10, frexw = 0.7) {
  
  tphi <- t(exp(model$phi))
  s <- rowSums(tphi)
  mat <- tphi / s # normed by columns of beta now.
  
  ex <- apply(mat, 2, rank) / nrow(mat)
  fr <- apply(tphi, 2, rank) / nrow(mat)
  frex <- 1 / (frexw / ex + (1 - frexw) / fr)
  index <- apply(tphi, 2, order, decreasing = TRUE)[1:top_n, ]
  
  out <- vector(length = ncol(tphi))
  for (i in seq_len(ncol(frex))) {
    out[i] <- sum(frex[index[, i], i])
  }
  
  return(mean(out))
}
```

We can now use this plot to evaluate the different models.

```{r}
models_df_metrics <- models_df |>
  mutate(semantic_coherence = map_dbl(model, semantic_coherence),
         exclusivity = map_dbl(model, exclusivity),
         divergence = map_dbl(model, divergence))

models_df_metrics |>
  select(-model) |>
  pivot_longer(-k, names_to = "metric") |>
  ggplot(aes(x = k, value, color = metric)) +
  geom_line(linewidth = 1.5, alpha = 0.7, show.legend = FALSE) +
  scale_x_continuous(breaks = scales::pretty_breaks()) +
  facet_wrap(~metric, scales = "free_y") +
  labs(x = "K (number of topics)",
       y = NULL,
       title = "Model diagnostics by number of topics",
       subtitle = "Higher = Better")
```


## Issues

- Often misused for text classification - topic models are for corpus exploration!
- Hardly ever validated (see [oolong](https://gesistsa.github.io/oolong/))
- Humans have the tendency to read meaning into meaningless patterns

## Learn more

:::: {.columns}
::: {.column width="70%"}
- Wouter van Atteveldt, Damian Trilling & Carlos Arcila: *Computational Analysis of Communication*, https://cssbook.net/
:::

::: {.column width="30%"}
![](https://cssbook.net/content/img/cover.jpg)
:::
::::

# Word Embeddings
## What are Word Embeddings?

> You shall know a word by the company it keeps!
> — @firth_synopsis_1968

- A different way of representing language
- Co-occurrences of words in the same text and constructs a representation of language by using dimension reduction techniques
- Similar to Principal Component Analysis, Factor Analysis, Multidimensional scaling or topic modelling
- This way, we charge words with "knowledge" about their meaning

## Why use Word Embeddings?

- Reduces the number of dimensions: from document-feature-matrix to document-embedding-matrix
- Analyses are far less expensive (however, the encoding step might be)
- The embeddings reflect the computer's understanding of language that it learned from the training corpus
- Less sensible to misspellings, synonyms and homonyms

## Training our own embeddings

Based on [Supervised Machine Learning for Text Analysis in R](https://smltar.com/embeddings.html)

```{r}
#| message: false
library(widyr)
library(irlba)
```

We can use the plenary speeches from the German Bundestag again and add word counts

```{r, cache.lazy=FALSE}
plenary_speeches_tidy_clean <- bundestag18 |>
  unnest_tokens(output = "word", input = "text") |> 
  add_count(word) |> # add count
  filter(n >= 50) |> # remove rare words
  select(-n)
```

For this analysis, we need to transform the data into a nested format, so the code below will know which words belong to the same speech:

```{r, cache.lazy=FALSE}
nested_words <- plenary_speeches_tidy_clean |>
  nest(words = c(word), .by = doc_id)

nested_words |> 
  select(doc_id, words) |> 
  head()
```

## Sliding windows

We cook up a small slide windows function:

```{r}
safe_mutate <- safely(mutate)  # create a safe mutate function (i.e., that does not error)

#function to create sliding windows of a given size
slide_windows <- function(tbl, window_size) {  
  # use the slider package to create the windows, This creates a list with one tibble per element
  skipgrams <- slider::slide(  
    tbl,  # input table
    .f = ~.x,  # slide over the only column in the tbl
    .after = window_size - 1,  # inlcude the current word, plus x next words
    .step = 1,  # number of words to shift forward
    .complete = TRUE  # include only complete windows
  )
  
  out <- map(seq_along(skipgrams),
             function(i) safe_mutate(skipgrams[[i]], window_id = i))  # add the window id to the skipgrams
  
  # wrangle output into the right format
  out |> 
    transpose() |>
    pluck("result") |>
    compact() |>
    bind_rows()
}
```

What this does is to put words into word windows, where each window consists of 3 words.
Let's have a look:

```{r}
toy_data <- tribble(
  ~id, ~text,
  1L, "I like cats. Cats are the best pets.",
  2L, "I like dogs. Dogs are the best pets."
)
toy_data |> 
  unnest_tokens("word", "text") |> 
  nest(words = c(word), .by = id) |>
  mutate(words = map(words, function(w) slide_windows(w, 3L))) |>
  unnest(words)
```

## Pointwise mutual information (PMI)

PMI: expresses how statistically likely it is that two words co-occur in the same window:

```{r}
# since this takes a long time, I save the results in case I want to run it again
if (!file.exists("data/R-03/speeches_pmi.rds")) {
  plan(multisession)  ## for parallel processing
  
  tidy_pmi <- nested_words |>
    mutate(words = future_map(words, function(w) slide_windows(w, 4L), .progress = TRUE)) |>
    unnest(words) |>
    unite(window_id, doc_id, window_id) |>
    pairwise_pmi(word, window_id)
  saveRDS(tidy_pmi, "data/R-03/speeches_pmi.rds")
} else {
  tidy_pmi <- readRDS("data/R-03/speeches_pmi.rds")
}
tidy_pmi
```

To get to the embedding vectors of the words, we use **singular value decomposition** (SVD). 
SVD is a method for dimensionality reduction.
In this case, we use `widely_svd`, which slightly obscures what dimensions will be reduced.
Let's make the data wide first to understand the step better:

```{r}
tidy_pmi |> 
  pivot_wider(id_cols = item1, names_from = item2, values_from = pmi)
```

Using SVD, we reduce this matrix to one with 100 dimensions/columns:

```{r, cache.lazy=FALSE}
tidy_word_vectors <- tidy_pmi |>
  widely_svd(
    item = item1, 
    feature = item2, 
    value = pmi,
    nv = 100, 
    maxit = 1000
  )

tidy_word_vectors |> 
  pivot_wider(id_cols = item1, names_from = dimension, values_from = value)
```

## Test our embeddings

- Every word is now expressed through the relation to all other words in the data
- We can find the words most similar with a nearest neighbors function

```{r}
nearest_neighbors <- function(tbl, word, n = 15) {
  m <- cast_dfm(tbl, item1, dimension, value) # transform to wide
  comp <- quanteda::dfm_subset(m, quanteda::docid(m) == word) # extract values for the provided word
  if (nrow(comp) < 1L) stop("word ", word, " not found in the embedding")
  sim <- proxyC::simil(comp, m, method = "cosine") # calculate cosine similarity
  rank <- order(as.numeric(sim), decreasing = TRUE)[seq_len(n + 1)] # get the n highest values plus original word
  
  sim[1, rank] |> 
    as_tibble(rownames = "neighbor")
}
```

We can check a couple of examples:

```{r}
#| error: true
nearest_neighbors(tidy_word_vectors, "damen")
nearest_neighbors(tidy_word_vectors, "asyl")
nearest_neighbors(tidy_word_vectors, "russland")
nearest_neighbors(tidy_word_vectors, "deutschland")
```

This seems to work really well, and we can think of a number of things we could do with this function:

- find which words are used interchangeably to reduce features with advanced pre-processing
- create or extend a dictionary or search string using the synonyms found
- explore which words are used very often together

## Doing Math with embeddings

The embeddings also make it possible to do math with language.
The iconic example is king – man + woman = queen.
Let's see if our embeddings can do a similar thing:

```{r}
m <- cast_dfm(tidy_word_vectors, item1, dimension, value) # transform to wide

w1 <- quanteda::dfm_subset(m, quanteda::docid(m) == "bürger") # extract values for the provided word
w2 <- quanteda::dfm_subset(m, quanteda::docid(m) == "mann") # extract values for the provided word
comp <- w1 - w2

sim <- proxyC::simil(comp, m, method = "cosine") # calculate cosine similarity
rank <- order(as.numeric(sim), decreasing = TRUE)[seq_len(15)] # get the n highest values plus original word

sim[1, rank] |> 
  as_tibble(rownames = "neighbor")
```

It actually works as bürger - mann = bürgerinnen as the second highest value.

## Issues

- to get good embeddings, you need enormous datasets
- encoding embeddings still follows the bag-of-words approach, where words are all treated the same no matter where they appeared


# generative Large Language Models
## What are generative Large Language Models?

- taken embeddings to the next level with enormous amounts of training data
- trained to predict/generate next word in a conversation
- surprisingly good at emulating humans to complete tasks
- can be controlled with natural language
- have become very popular through ChatGPT (a browser interface to communicate with OpenAI's GPT models)

## Why use gLLMs? 

::: {.smaller}
- cheaper to use 💸 (experts > research assistants > crowd workers > gLLMs)
- more accurate 🧠 (experts > research assistants > gLLMs > crowd workers)
- better at annotating complex categories 🤔 (human* > gLLM > LLM > SML)
- can use contextual information (human* > gLLM > LLM > ❌ SML)
- multilingual^[but still need of validation!]
- easier* to use: plain English prompting

[e.g., @GilardiChatGPT2023;@Heseltine_Hohenberg_2023;@Zhong_ChatGPT_2023;@Törnberg_2023]
:::

## How do you work with them - simple example

:::: {.columns}

::: {.column width="50%"}
### OpenAI GPT via API

```{r askgpt}
library(askgpt)
options(askgpt_chat_model = "gpt-4")
askgpt("What are generative Large Language Models?")
```
:::

::: {.column .fragment width="50%"}
### Running model locally through Ollama

```{r rollama}
library(rollama)
query("What are generative Large Language Models?", model = "llama3:8b")
```
:::

::::


## Classification--Strategies

::: {style="font-size: 50%;"}
| Prompting Strategy | Example Structure |
|--------------------|--------------------|
| Zero-shot          | `{"role": "system", "content": "Text of System Prompt"},`<br>`{"role": "user", "content": "(Text to classify) + classification question"}` |
| One-shot           | `{"role": "system", "content": "Text of System Prompt"},`<br>`{"role": "user", "content": "(Example text) + classification question"},`<br>`{"role": "assistant", "content": "Example classification"},`<br>`{"role": "user", "content": "(Text to classify) + classification question"}` |
| Few-shot           | `{"role": "system", "content": "Text of System Prompt"},`<br>`{"role": "user", "content": "(Example text) + classification question"},`<br>`{"role": "assistant", "content": "Example classification"},`<br>`{"role": "user", "content": "(Example text) + classification question"},`<br>`{"role": "assistant", "content": "Example classification"},`<br>`. . . more examples`<br>`{"role": "user", "content": "(Text to classify) + classification question"}` |
| Chain-of-Thought   | `{"role": "system", "content": "Text of System Prompt"},`<br>`{"role": "user", "content": "(Text to classify) + reasoning question"},`<br>`{"role": "assistant", "content": "Reasoning"},`<br>`{"role": "user", "content": "Classification question"}` |
: Prompting strategies {.striped .hover tbl-colwidths="[15,85]"}
:::

See: @weber2023evaluation


## Classification--Zero-shot {.scrollable}

```{r}
q <- tribble(
  ~role,    ~content,
  "system", "You assign texts into categories. Answer with just the correct category.",
  "user",   "text: the pizza tastes terrible\ncategories: positive, neutral, negative"
)
query(q)
```

## Classification--One-shot {.scrollable}

```{r}
q <- tribble(
  ~role,    ~content,
  "system", "You assign texts into categories. Answer with just the correct category.",
  "user", "text: the pizza tastes terrible\ncategories: positive, neutral, negative",
  "assistant", "Category: Negative",
  "user", "text: the service is great\ncategories: positive, neutral, negative"
)
query(q)
#> 
#> ── Answer ────────────────────────────────────────────────────────
#> Category: Positive
```

:::{.fragment}

Neat effect: change the output structure

```{r}
#| classes: .fragement .fade-in
#| code-line-numbers: "5,11"
q <- tribble(
  ~role,    ~content,
  "system", "You assign texts into categories. Answer with just the correct category.",
  "user", "text: the pizza tastes terrible\ncategories: positive, neutral, negative",
  "assistant", "{'Category':'Negative','Confidence':'100%','Important':'terrible'}",
  "user", "text: the service is great\ncategories: positive, neutral, negative"
)
answer <- query(q)
#> 
#> ── Answer ────────────────────────────────────────────────────────
#> {'Category':'Positive','Confidence':'100%','Important':'great'}
```

:::

## Classification--Few-shot

```{r}
q <- tribble(
  ~role,    ~content,
  "system", "You assign texts into categories. Answer with just the correct category.",
  "user", "text: the pizza tastes terrible\ncategories: positive, neutral, negative",
  "assistant", "Category: Negative",
  "user", "text: the service is great\ncategories: positive, neutral, negative",
  "assistant", "Category: Positive",
  "user", "text: I once came here with my wife\ncategories: positive, neutral, negative",
  "assistant", "Category: Neutral",
  "user", "text: I once ate pizza\ncategories: positive, neutral, negative"
)
query(q)
```

## Classification--Chain-of-Thought {.scrollable}

```{r}
#| code-line-numbers: "4,11-12"
q_thought <- tribble(
  ~role,    ~content,
  "system", "You assign texts into categories. ",
  "user",   "text: the pizza tastes terrible\nWhat sentiment (positive, neutral, or negative) would you assign? Provide some thoughts."
)
output_thought <- query(q_thought)
```

:::{.fragment}
Now we can use these thoughts in classification

```{r}
#| echo: true
#| eval: false
#| code-line-numbers: "6"
q <- tribble(
  ~role,    ~content,
  "system", "You assign texts into categories. ",
  "user",   "text: the pizza tastes terrible\nWhat sentiment (positive, neutral, or negative) would you assign? Provide some thoughts.",
  "assistant", pluck(output_thought, "message", "content"),
  "user",   "Now answer with just the correct category (positive, neutral, or negative)"
)
query(q)
```

Full tutorial (including how to batch annotate): <https://jbgruber.github.io/rollama/articles/annotation.html>
:::

# References
